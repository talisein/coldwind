  * Lurker should use a std::set rather than std::list [Easy] (Just
    make sure you test this. The hash should be based on just the
    thread, not every element. [Easy+]

  * Record download statistics from curl (Time, Bytes), populate back
    to UI [Medium] [Mostly done]

  * Decode curl's error messages and put on std:cerr [Easy] [Mostly
    done]

  * Redirect std::cout and std::cerr to an optional popup message log
    [Hard] (There's actually a Glib API for this in g_log)

  * Capture libxml errors and do the right thing. [Easy] [Partly done]

  * Overlay some lurker info text on the image [Medium]
  ** Or some sort of visual lurker feedback. I dunno. [Hard]
  *** Well, there's some std::cout things now. [Partly done]

  * The hasher shouldn't try to hash anything larger than 4chan's
    maximum image size [Easy]

  * In Glib 3.32, use GResources. And it looks like GApplication will
    finally land. [Longterm]

  * Store the hash cache to disk [Medium]

  * Accept options for number of hashing threads & curl connections [Easy]

  * Don't lock the UI during download. There should be a creative use
    of Glib::Mutex as an alternative to try_download() testing. [Medium]

  * Allow user to specify thread directory name [Easy]
  ** Provide autocompletion [Medium+]

  * set_sensitivity(false) on the spinner when lurk-to-404 [Easy]

  * In the parser, use libcurl instead of libxml2 to fetch the HTML [Hard]
  ** Use CURLOPT_TIMECONDITION = CURL_TIMECOND_IFMODSINCE and
     CURLOPT_TIMEVALUE = long unix timestamp
  *** Only the lurker uses this functionality
  ** The HTML parsing can probably be done in CURLOPT_WRITEFUNCTION
  ** Might as well cache the connection
  ** Rewrite the 404 logic

